{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cf5CmXQCZyF1"
   },
   "source": [
    "# Capstone 3 Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cc0lyndRYxYY"
   },
   "source": [
    "**The Data Science Method**  \n",
    "\n",
    "1.   Problem Identification \n",
    "\n",
    "2.   Data Wrangling \n",
    "  * Data Collection \n",
    "   * Data Organization\n",
    "  * Data Definition \n",
    "  * Data Cleaning\n",
    " \n",
    "3.   Exploratory Data Analysis\n",
    " * Build data profile tables and plots\n",
    "        - Outliers & Anomalies\n",
    " * Explore data relationships\n",
    " * Identification and creation of features\n",
    "\n",
    "4.   Pre-processing and Training Data Development\n",
    "  * Create dummy or indicator features for categorical variables\n",
    "  * Standardize the magnitude of numeric features\n",
    "  * Split into testing and training datasets\n",
    "  * Apply scaler to the testing set\n",
    "  \n",
    "5.   **Modeling**\n",
    "  * Fit Models with Training Data Set\n",
    "  * Review Model Outcomes â€” Iterate over additional models as needed.\n",
    "  * Identify the Final Model\n",
    "\n",
    "6.   Documentation\n",
    "  * Review the Results\n",
    "  * Present and share your findings - storytelling\n",
    "  * Finalize Code \n",
    "  * Finalize Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9YQUNzQ_PqR9"
   },
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ry6WPL5eZyF3"
   },
   "outputs": [],
   "source": [
    "#load python packages\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "pd.set_option('display.float_format', lambda x: '%.4f' % x) #get rid of scientific notations\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import time\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from statsmodels.tsa.api import VAR\n",
    "from IPython.display import Image\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\tc18f\\\\Desktop\\\\springboard\\\\Capstone Three\\\\data\\\\processed'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# switch to processed data folder\n",
    "os.chdir('C:\\\\Users\\\\tc18f\\\\Desktop\\\\springboard\\\\Capstone Three\\\\data\\\\processed\\\\')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the combined csv file\n",
    "df = pd.read_csv('combined.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check info\n",
    "df['Date'] = pd.to_datetime(df.Date)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# break the df into 4 of them indicating the number of bedrooms and drop the county and bedrooms column\n",
    "df1 = df.loc[df['Bedrooms']==1]\n",
    "df1 = df1.sort_values(['Date','Zipcode'])\n",
    "df2 = df.loc[df['Bedrooms']==2]\n",
    "df2 = df2.sort_values(['Date','Zipcode'])\n",
    "df3 = df.loc[df['Bedrooms']==3]\n",
    "df3 = df3.sort_values(['Date','Zipcode'])\n",
    "df4 = df.loc[df['Bedrooms']==4]\n",
    "df4 = df4.sort_values(['Date','Zipcode'])\n",
    "df1.tail() # previous when the date wasn't in date time and sort by date will see 2019 in tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess for VARMAX model\n",
    "need the values to be differenced first then cube root it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that takes in the df, and the number of diff value\n",
    "def data_trans(df, val_name):\n",
    "    temp = pd.DataFrame({})\n",
    "    # add Zipcode as columns and its values to temp\n",
    "    for zipcode in list(df.Zipcode.unique()):\n",
    "        temp[zipcode] = list(df[df['Zipcode']==zipcode].Value)\n",
    "    # difference the data\n",
    "    temp = temp.diff().dropna()\n",
    "    # add Date column to so we can melt it, starting date is 1996-2-29 periods=293 since it's differenced and lost 1 month\n",
    "    temp['Date'] = pd.date_range('1996-02-29', periods=292, freq='M')\n",
    "    # melt and sort\n",
    "    temp_melt = pd.melt(temp, id_vars=['Date'], var_name='Zipcode', value_name=val_name)\n",
    "    temp_sort = temp_melt.sort_values(['Date','Zipcode'])\n",
    "    # have the Values in float since it has imaginary number\n",
    "    temp_sort[val_name] = temp_sort[val_name].astype('float64')\n",
    "    # set the Date as index\n",
    "    temp_sort.set_index('Date', inplace=True)\n",
    "    return temp_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1t = data_trans(df1, 'OneBR')\n",
    "df2t = data_trans(df2, 'TwoBR')\n",
    "df3t = data_trans(df3, 'ThreeBR')\n",
    "df4t = data_trans(df4, 'FourBR')\n",
    "dft = df1t\n",
    "dft['TwoBR'] = df2t.TwoBR\n",
    "dft['ThreeBR'] = df3t.ThreeBR\n",
    "dft['FourBR'] = df4t.FourBR\n",
    "dft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set the endog (training data)\n",
    "endog = dft.loc['1996-2-29':'2015-05-31']\n",
    "endog.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the index, previously getting warning messages regarding no frequency\n",
    "endog[endog['Zipcode']==90004].index #need to set the frequency for subsets of endog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# iterate zipcode to subset and model/fit/predict\n",
    "# create dfs to store values\n",
    "pred_df1 = pd.DataFrame({'Date':pd.date_range('2015-06-30', periods=60, freq='M')})\n",
    "pred_df2 = pred_df1\n",
    "pred_df3 = pred_df1\n",
    "pred_df4 = pred_df1\n",
    "# iterate thru zipcode and get the values stored; order=(1,0) trend='n' since the data is stationary\n",
    "start = time.time()\n",
    "for zipcode in list(dft.Zipcode.unique()):\n",
    "    # get the endog by subsetting the dft with specific zipcode and collumns other thatn zipcode\n",
    "    endog_sub = endog[endog['Zipcode']==zipcode][[i for i in list(dft.columns)][1:]]\n",
    "    endog_sub.index.freq = 'M' #set the datetime frequency\n",
    "    # train/fit the model\n",
    "    model = sm.tsa.VARMAX(endog_sub, order=(1,0), trend='n')\n",
    "    result = model.fit(maxiter=1000, disp=False)\n",
    "    # forecast\n",
    "    pred = result.predict(start='2015-06-30', end='2020-05-31')\n",
    "    # append data to dataframes\n",
    "    pred_df1[zipcode] = list(pred.OneBR)\n",
    "    pred_df2[zipcode] = list(pred.TwoBR)\n",
    "    pred_df3[zipcode] = list(pred.ThreeBR)\n",
    "    pred_df4[zipcode] = list(pred.FourBR)\n",
    "# stop timer\n",
    "end = time.time()\n",
    "fit_time = (end-start)\n",
    "#check the fit time in min\n",
    "int(fit_time/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check what the prediction dataframes look like\n",
    "display(pred_df1.head(2))\n",
    "display(pred_df1.tail(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform predictions back to original format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to inverse the diff by having the first value\n",
    "def diff_inv(series_diff, first_value):\n",
    "    series = np.r_[first_value, series_diff].cumsum().astype('float64')\n",
    "    return series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test the function above\n",
    "x = pd.DataFrame({'test':[3,7,2,6,8,0,5,0,4,9,5]})\n",
    "x['diff'] = x.diff()\n",
    "x_diff = x['diff'].dropna() # x_diff is a dataframe with single column\n",
    "display(diff_inv(x_diff, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test = df1[df1['Zipcode']==90004]\n",
    "test.iloc[-61] # we need 2015-05-31's Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that will revert back the prediction values\n",
    "def pred_trans(pred_df, original_df, val_name):\n",
    "    temp = pd.DataFrame({}) #create dfs to store values\n",
    "    restored = pd.DataFrame({'Date':pd.date_range('2015-05-31', periods=61, freq='M')})\n",
    "    for col in list(pred_df.columns)[1:]:\n",
    "        # inverse the diff()\n",
    "        original_df_sub = original_df[original_df['Zipcode']==col]\n",
    "        first_value = original_df_sub.iloc[-61].Value\n",
    "        restored[col] = diff_inv(temp[col], first_value)\n",
    "    # melt\n",
    "    temp_melt = pd.melt(restored, id_vars=['Date'], var_name='Zipcode', value_name=val_name)\n",
    "    # make sure date is datetime\n",
    "    temp_melt['Date'] = pd.to_datetime(temp_melt.Date)\n",
    "    # sort\n",
    "    temp_sort = temp_melt.sort_values(['Date','Zipcode'])\n",
    "    # have the Values in int so it's easier to read and compared to original values\n",
    "    temp_sort[val_name] = temp_sort[val_name].astype('int64')\n",
    "    return temp_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# transform and compile the prediction\n",
    "pred_dft = pred_trans(pred_df1, df1, 'OneBR')\n",
    "pred_dft2 = pred_trans(pred_df2, df2, 'TwoBR')\n",
    "pred_dft3 = pred_trans(pred_df3, df3, 'ThreeBR')\n",
    "pred_dft4 = pred_trans(pred_df4, df4, 'FourBR')\n",
    "pred_dft['TwoBR'] = pred_dft2.TwoBR\n",
    "pred_dft['ThreeBR'] = pred_dft3.ThreeBR\n",
    "pred_dft['FourBR'] = pred_dft4.FourBR\n",
    "pred_dft.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the restored prediciton to csv file\n",
    "pred_dft.to_csv('varmax_pred5Dn.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the transformed data\n",
    "pred_dft = pd.read_csv('varmax_pred5Dn.csv')\n",
    "pred_dft['Date'] = pd.to_datetime(pred_dft.Date) # make sure Date column is datetime!\n",
    "pred_dft.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try plotting the mean/medians of test and pred, we will need to define functions\n",
    "# define a function that takes in the prediction dataframe and return the medians\n",
    "def median_calc(data, nobs, val_column):\n",
    "    p_medians = []\n",
    "    for i in range(nobs): \n",
    "        median = data.sort_values(['Date',val_column]).iloc[240+481*i][val_column]\n",
    "        p_medians.append(int(median))\n",
    "    return p_medians\n",
    "# define a function that takes in the prediction dataframe and return the means\n",
    "def mean_calc(data, val_column):\n",
    "    p_means = []\n",
    "    for i in list(data.Date.unique()): # get the dates to iterate\n",
    "        mean = data[data['Date']==i][val_column].mean() # get the mean\n",
    "        p_means.append(int(mean))\n",
    "    return p_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the z_avg csv which has zillow's averages\n",
    "z_avg = pd.read_csv('z_avg.csv')\n",
    "z_avg['Date'] = pd.to_datetime(z_avg.Date)\n",
    "z_avg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's compiles the means and medians to form a new dataframe and plot on seaborn\n",
    "z_df = pd.DataFrame({'Date':pd.date_range('1996-01-31', periods=293, freq='M')})\n",
    "z_df['z_median_OneBR'] = z_avg.median1\n",
    "z_df['z_median_TwoBR'] = z_avg.median2\n",
    "z_df['z_median_ThreeBR'] = z_avg.median3\n",
    "z_df['z_median_FourBR'] = z_avg.median4\n",
    "z_df['z_mean_OneBR'] = z_avg.mean1\n",
    "z_df['z_mean_TwoBR'] = z_avg.mean2\n",
    "z_df['z_mean_ThreeBR'] = z_avg.mean3\n",
    "z_df['z_mean_FourBR'] = z_avg.mean4\n",
    "# do the same for prediction\n",
    "pred_date_num = pred_dft.Date.nunique()\n",
    "p_df = pd.DataFrame({'Date':pd.date_range('2015-05-31', periods=pred_date_num, freq='M')})\n",
    "p_df['p_median_OneBR'] = median_calc(pred_dft, pred_date_num, 'OneBR')\n",
    "p_df['p_median_TwoBR'] = median_calc(pred_dft, pred_date_num, 'TwoBR')\n",
    "p_df['p_median_ThreeBR'] = median_calc(pred_dft, pred_date_num, 'ThreeBR')\n",
    "p_df['p_median_FourBR'] = median_calc(pred_dft, pred_date_num, 'FourBR')\n",
    "p_df['p_mean_OneBR'] = mean_calc(pred_dft, 'OneBR')\n",
    "p_df['p_mean_TwoBR'] = mean_calc(pred_dft, 'TwoBR')\n",
    "p_df['p_mean_ThreeBR'] = mean_calc(pred_dft, 'ThreeBR')\n",
    "p_df['p_mean_FourBR'] = mean_calc(pred_dft, 'FourBR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# melt and we need the following columns for seaborn: Date, measure_type, Value, Bedrooms, Source\n",
    "z_melt = pd.melt(z_df, ['Date'])\n",
    "p_melt = pd.melt(p_df, ['Date'])\n",
    "sea_df = pd.concat([z_melt, p_melt])\n",
    "sea_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create a list to add the measure type, bedrooms, and sources\n",
    "measure_list=[]\n",
    "for i in sea_df.variable:\n",
    "    if 'median' in i:\n",
    "        measure_list.append('median')\n",
    "    else:\n",
    "        measure_list.append('mean')\n",
    "# do the same for number of bedrooms\n",
    "BR_list=[]\n",
    "for i in sea_df.variable:\n",
    "    if 'One' in i:\n",
    "        BR_list.append(1)\n",
    "    if 'Two' in i:\n",
    "        BR_list.append(2)\n",
    "    if 'Three' in i:\n",
    "        BR_list.append(3)\n",
    "    if 'Four' in i:\n",
    "        BR_list.append(4)\n",
    "# source        \n",
    "source_list=[]\n",
    "for i in sea_df.variable:\n",
    "    if 'z_m' in i:\n",
    "        source_list.append('zillow')\n",
    "    else:\n",
    "        source_list.append('varmax')\n",
    "# add the columns to sea_df\n",
    "sea_df['measure_type'] = measure_list\n",
    "sea_df['Bedrooms'] = BR_list\n",
    "sea_df['Source'] = source_list\n",
    "display(sea_df.head(3))\n",
    "sea_df.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot the means using seaborn\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "sea_mean = sea_df[sea_df['measure_type']=='mean'] #subset with only the means\n",
    "fig, ax = plt.subplots(figsize=(16, 8)) # set the figure size and ax to graph on\n",
    "sea = sns.lineplot(data=sea_mean, x=\"Date\", y=\"value\", hue=\"Source\", style=\"Bedrooms\", ax=ax)\n",
    "sea.axes.set_title(\"Zillow vs VARMAX means\",fontsize=18)\n",
    "sea.set_xlabel(\"Date\",fontsize=14)\n",
    "sea.set_ylabel(\"Value (mil)\",fontsize=14)\n",
    "plt.savefig('C:\\\\Users\\\\tc18f\\\\Desktop\\\\springboard\\\\Capstone Three\\\\figures\\\\Zillow_vs_VARMAX_meansDn.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the median using seaborn\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "sea_mean = sea_df[sea_df['measure_type']=='median'] #subset with only the means\n",
    "fig, ax = plt.subplots(figsize=(16, 8)) # set the figure size and ax to graph on\n",
    "sea = sns.lineplot(data=sea_mean, x=\"Date\", y=\"value\", hue=\"Source\", style=\"Bedrooms\", ax=ax)\n",
    "sea.axes.set_title(\"Zillow vs VARMAX medians\",fontsize=18)\n",
    "sea.set_xlabel(\"Date\",fontsize=14)\n",
    "sea.set_ylabel(\"Value (mil)\",fontsize=14)\n",
    "plt.savefig('C:\\\\Users\\\\tc18f\\\\Desktop\\\\springboard\\\\Capstone Three\\\\figures\\\\Zillow_vs_VARMAX_mediansDn.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions for the first 2 years seems okay, we shall compare the model scores based on each 12 months of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dft.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's make a test dataframe\n",
    "def test_data_form(df, BR_num, val_name):\n",
    "    temp=pd.DataFrame({})\n",
    "    for zipcode in pred_dft.Zipcode.unique():\n",
    "        df_sub = df[(df['Zipcode']==zipcode) & (df['Bedrooms']==BR_num)] #subset by zipcode and num of bedroom\n",
    "        df_sub = df_sub[df_sub['Date']>='2015-05-31'] # get corresponding date range\n",
    "        temp['Date'] = pd.date_range('2015-05-31', periods=len(df_sub), freq='M')\n",
    "        temp[zipcode]=list(df_sub.Value)\n",
    "    # melt\n",
    "    temp_melt = pd.melt(temp, id_vars=['Date'], var_name='Zipcode', value_name=val_name)\n",
    "    # make sure date is datetime\n",
    "    temp_melt['Date'] = pd.to_datetime(temp_melt.Date)\n",
    "    # sort\n",
    "    temp_sort = temp_melt.sort_values(['Date','Zipcode'])\n",
    "    # have the Values in int so it's easier to read and compared to original values\n",
    "    temp_sort[val_name] = temp_sort[val_name].astype('int64')\n",
    "    return temp_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# make a compiled dataframe\n",
    "test = test_data_form(df,1, 'OneBR')\n",
    "test2 = test_data_form(df,2, 'TwoBR')\n",
    "test3 = test_data_form(df,3, 'ThreeBR')\n",
    "test4 = test_data_form(df,4, 'FourBR')\n",
    "test['TwoBR'] = test2.TwoBR\n",
    "test['ThreeBR'] = test3.ThreeBR\n",
    "test['FourBR'] = test4.FourBR\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dft.set_index('Date',inplace=True)\n",
    "test.set_index('Date',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# calculate the MAEs and store to csv\n",
    "model_list=[]\n",
    "MAE_list=[]\n",
    "for i in range(0,5):\n",
    "    test_sub = test.iloc[i*481:(1+i)*481*12]\n",
    "    pred_sub = pred_dft.iloc[i*481:(1+i)*481*12]\n",
    "    model_list.append('VARMAX5')\n",
    "    MAE = mean_absolute_error(test_sub, pred_sub)\n",
    "    MAE_list.append(MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_score = pd.DataFrame({\n",
    "    'Model': model_list,\n",
    "    'Cycle': ['15to16', '16to17', '17to18','18to19','19to20'],\n",
    "    'MAE':MAE_list\n",
    "})\n",
    "model_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "GuidedCapstoneStep2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": "0",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
